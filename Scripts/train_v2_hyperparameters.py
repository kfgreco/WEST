#!/usr/bin/env python3
# ==============================================================
# WEST: Model Training Script
# --------------------------------------------------------------
# Trains the WEST model using a single hyperparameter configuration.
# This script is called automatically by hyperparameter search jobs.
# ==============================================================

import argparse
import os
import time
import json
import torch
import torch.nn as nn
import torch.distributed as dist
import pandas as pd
import numpy as np
from datetime import timedelta
from torch.utils.data import DataLoader
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from sklearn.metrics import roc_auc_score
from transformers import (
    get_cosine_schedule_with_warmup,
    get_linear_schedule_with_warmup
)

from .model_v2 import PatientTransformer
from .loss import contrastive_loss
from .patient_dataset import PatientDataset


# ----------------------------------------------------------------------
# Exponential Moving Average (EMA) for Model Parameters
# ----------------------------------------------------------------------

class EMA:
    """Implements Exponential Moving Average (EMA) for model parameters."""

    def __init__(self, model, decay=0.9):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_avg = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data
                self.shadow[name] = new_avg.clone()

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}


# ----------------------------------------------------------------------
# Argument Parsing
# ----------------------------------------------------------------------

def _override_args_from_config(args: argparse.Namespace, config_path: str) -> argparse.Namespace:
    """Override argparse args with values from a JSON config if present."""
    if config_path is None:
        return args
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"--config file not found: {config_path}")

    with open(config_path, "r") as f:
        cfg = json.load(f)

    # Only set keys that already exist on args to avoid surprises
    for k, v in cfg.items():
        if hasattr(args, k):
            setattr(args, k, v)
    return args


def parse_args():
    """Parse command line arguments for WEST training."""
    parser = argparse.ArgumentParser(description="Train WEST Model")

    # Config file (added)
    parser.add_argument("--config", type=str, default=None,
                        help="Path to JSON configuration file generated by hyperparameter search.")

    # Data parameters
    parser.add_argument("--data_path", type=str, default="./Patients_1204")
    parser.add_argument("--summary_file_name", type=str, default="patient_summary.csv")
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--remove_gold_negative", action="store_true")

    # Model parameters
    parser.add_argument("--hidden_dim", type=int, default=256)
    parser.add_argument("--num_heads", type=int, default=8)
    parser.add_argument("--num_layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.3)
    parser.add_argument("--output_type", type=str, default="mean")

    # Training parameters
    parser.add_argument("--num_epochs", type=int, default=50)
    parser.add_argument("--learning_rate", type=float, default=2e-4)
    parser.add_argument("--weight_decay", type=float, default=0.05)
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    parser.add_argument("--warmup_steps", type=int, default=1000)
    parser.add_argument("--print_every", type=int, default=1)
    parser.add_argument("--early_stopping_patience", type=int, default=10)

    # Device and seed
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--seed", type=int, default=42)

    # Model saving
    parser.add_argument("--save_dir", type=str, default="./models")
    parser.add_argument("--model_name", type=str, default="patient_transformer_single.pt")

    # Distributed training
    parser.add_argument("--local-rank", type=int, default=-1)
    parser.add_argument("--world-size", type=int, default=-1)

    # Contrastive learning
    parser.add_argument("--temperature", type=float, default=0.07)
    parser.add_argument("--contrastive_weight", type=float, default=0.1)
    parser.add_argument("--pos_ratio", type=float, default=None)

    # EMA
    parser.add_argument("--ema_decay", type=float, default=0.9)
    parser.add_argument("--use_ema", action="store_true")

    # Sequence and augmentation parameters
    parser.add_argument("--max_seq_len", type=int, default=50)
    parser.add_argument("--top_k", type=int, default=50)
    parser.add_argument("--use_augmentation", action="store_true")

    # Scheduler
    parser.add_argument("--scheduler_type", type=str, default="linear", choices=["cosine", "linear"])

    # Label column
    parser.add_argument("--label_column", type=str, default="FINALPAH")

    args = parser.parse_args()
    # Merge JSON config values (if provided) after CLI parse so CLI can still override defaults
    args = _override_args_from_config(args, args.config)
    return args


# ----------------------------------------------------------------------
# Model Training
# ----------------------------------------------------------------------

def train_model(args):
    """Train the WEST model using provided hyperparameters."""
    if args.local_rank in [-1, 0]:
        os.makedirs(args.save_dir, exist_ok=True)

    # Distributed setup
    if args.local_rank != -1:
        torch.cuda.set_device(args.local_rank)
        dist.init_process_group(backend="nccl")
        args.world_size = dist.get_world_size()

    torch.manual_seed(args.seed + max(args.local_rank, 0))
    device = torch.device(args.local_rank if args.local_rank != -1 else "cuda")

    # ------------------------------------------------------------------
    # Load embeddings and mapping
    # ------------------------------------------------------------------
    # These two files work together to link domain-specific concept identifiers
    # (e.g., CCS procedure codes) to their corresponding embedding vectors.
    #
    # 1. Mapping.csv
    #    - Columns:
    #        CODE:  The unique concept identifier (e.g., "CCS:1", "CCS:2", etc.)
    #        Index: The zero-based index that corresponds to the same concept’s
    #                row position in the embedding file.
    #    - Purpose:
    #        Provides a lookup table to translate between concept codes and
    #        their vector positions in the embedding matrix.
    #
    # 2. Embeddings.csv
    #    - Rows:
    #        Each row represents a single concept (aligned with Mapping.csv order).
    #    - Columns:
    #        V1, V2, V3, ... : Continuous numerical features forming an
    #        n-dimensional embedding vector.
    #    - Purpose:
    #        Encodes semantic relationships between concepts — concepts with
    #        similar meanings or contexts will have similar vector representations.

    datax = pd.read_csv('.../Transformer/Input/Embeddings.csv')
    mapping = pd.read_csv('.../Transformer/Input/Mapping.csv')
    code_embeddings = torch.tensor(datax.iloc[:, :].to_numpy(), dtype=torch.float32)
    
    # ------------------------------------------------------------------
    # Dataset and DataLoader setup
    # ------------------------------------------------------------------
    train_dataset = PatientDataset(
        data_dir=args.data_path,
        summary_file_name=args.summary_file_name,
        code_embeddings=code_embeddings,
        code_mapping=mapping,
        max_seq_len=args.max_seq_len,
        top_k=args.max_seq_len,
        training=True,
        pos_ratio=args.pos_ratio,
        use_augmentation=args.use_augmentation,
        remove_gold_negative=args.remove_gold_negative,
        label_column=args.label_column
    )

    val_dataset = PatientDataset(
        data_dir=args.data_path,
        summary_file_name=args.summary_file_name,
        code_embeddings=code_embeddings,
        code_mapping=mapping,
        max_seq_len=args.max_seq_len,
        top_k=args.max_seq_len,
        training=False,
        label_column=args.label_column
    )

    train_sampler = DistributedSampler(train_dataset) if args.local_rank != -1 else None
    val_sampler = DistributedSampler(val_dataset, shuffle=False) if args.local_rank != -1 else None

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=min(4, os.cpu_count()),
        pin_memory=True,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        sampler=val_sampler,
        num_workers=4,
        pin_memory=True
    )

    # ------------------------------------------------------------------
    # Model, optimizer, scheduler, and EMA
    # ------------------------------------------------------------------
    model = PatientTransformer(
        d_model=args.hidden_dim,
        nhead=args.num_heads,
        dropout=args.dropout,
        num_layers=args.num_layers,
        output_type=args.output_type
    ).to(device)

    if args.local_rank != -1:
        model = DDP(model, device_ids=[args.local_rank])

    ema = None
    if args.use_ema and args.local_rank in [-1, 0]:
        ema = EMA(model.module if args.local_rank != -1 else model, decay=args.ema_decay)
        ema.register()

    criterion = nn.BCELoss()
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )

    if args.scheduler_type == "cosine":
        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=args.warmup_steps,
            num_training_steps=len(train_loader) * args.num_epochs,
            num_cycles=0.5
        )
    else:
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=args.warmup_steps,
            num_training_steps=len(train_loader) * args.num_epochs
        )

    # ------------------------------------------------------------------
    # Training loop
    # ------------------------------------------------------------------
    best_val_auc_all = best_val_auc_fold1 = best_val_auc_fold2 = 0
    patience_counter = 0

    for epoch in range(args.num_epochs):
        if train_sampler:
            train_sampler.set_epoch(epoch)

        model.train()
        total_loss = 0
        train_predictions, train_labels = [], []

        if args.local_rank in [-1, 0]:
            print(f"\nEpoch {epoch + 1}/{args.num_epochs}")

        for batch_idx, batch in enumerate(train_loader):
            code_embeddings = batch["code_embeddings"].to(device)
            counts = batch["counts"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].float().unsqueeze(1).to(device)

            # Forward
            if args.contrastive_weight > 0:
                outputs, features = model(code_embeddings, counts, attention_mask)
            else:
                outputs = model(code_embeddings, counts, attention_mask, return_features=False)

            # Contrastive loss (if enabled)
            if args.use_ema and args.contrastive_weight > 0:
                ema.apply_shadow()
                model.eval()
                with torch.no_grad():
                    _, ema_features = model(code_embeddings, counts, attention_mask)
                model.train()
                ema.restore()
                cont_loss = contrastive_loss(features, ema_features, args.temperature)
            else:
                cont_loss = torch.tensor(0.0, device=device)

            # BCE loss
            bce_loss = criterion(outputs, labels)
            loss = bce_loss + args.contrastive_weight * cont_loss

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)
            optimizer.step()
            scheduler.step()

            if args.use_ema and args.local_rank in [-1, 0]:
                ema.update()

            train_predictions.extend(outputs.detach().cpu().numpy())
            train_labels.extend(labels.cpu().numpy())
            total_loss += loss.item()

        # ------------------------------------------------------------------
        # Validation
        # ------------------------------------------------------------------
        model.eval()
        if args.use_ema and args.local_rank in [-1, 0]:
            ema.apply_shadow()

        val_predictions, val_labels = [], []
        val_loss = 0

        with torch.no_grad():
            for batch in val_loader:
                code_embeddings = batch["code_embeddings"].to(device)
                counts = batch["counts"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["label"].float().unsqueeze(1).to(device)

                outputs, _ = model(code_embeddings, counts, attention_mask)
                val_loss += criterion(outputs, labels).item()
                val_predictions.extend(outputs.cpu().numpy())
                val_labels.extend(batch["gold_label"].cpu().numpy())

        if args.use_ema and args.local_rank in [-1, 0]:
            ema.restore()

        # ------------------------------------------------------------------
        # Compute AUC metrics (overall + folds)
        # ------------------------------------------------------------------
        summary_df = pd.read_csv(os.path.join(args.data_path, args.summary_file_name))
        summary_df.index = summary_df["ID"].astype(str)
        val_patient_ids = [str(pid) for pid in val_dataset.patient_ids]

        # Align patient IDs and extract fold assignments
        kfold_vals = np.array([
            summary_df.loc[pid]["kfold_2"] if pid in summary_df.index else np.nan
            for pid in val_patient_ids
        ])

        # Convert predictions and labels to arrays
        val_predictions = np.array(val_predictions).flatten()
        val_labels = np.array(val_labels).flatten()

        # Compute masks for each fold
        mask1 = kfold_vals == 1
        mask2 = kfold_vals == 2

        # Compute AUCs
        val_auc_all = roc_auc_score(val_labels, val_predictions)
        val_auc_fold1 = roc_auc_score(val_labels[mask1], val_predictions[mask1]) if mask1.any() else float("nan")
        val_auc_fold2 = roc_auc_score(val_labels[mask2], val_predictions[mask2]) if mask2.any() else float("nan")

        # ------------------------------------------------------------------
        # Logging and checkpointing
        # ------------------------------------------------------------------
        if args.local_rank in [-1, 0]:
            print(f"Validation AUC (All):    {val_auc_all:.4f}")
            print(f"Validation AUC (Fold 1): {val_auc_fold1:.4f}")
            print(f"Validation AUC (Fold 2): {val_auc_fold2:.4f}")

            def save_best_model(auc_value, best_auc, tag):
                if auc_value > best_auc:
                    model_to_save = model.module if args.local_rank != -1 else model
                    torch.save({
                        "epoch": epoch,
                        "model_state_dict": model_to_save.state_dict(),
                        "optimizer_state_dict": optimizer.state_dict(),
                        "scheduler_state_dict": scheduler.state_dict(),
                        "ema_state": ema.shadow if args.use_ema else None,
                        "best_val_auc": auc_value,
                    }, f"{args.save_dir}/best_{tag}_{args.model_name}")
                    print(f"New best AUC for {tag}: {auc_value:.4f}")
                    return auc_value, 0
                return best_auc, patience_counter + 1

            best_val_auc_fold1, patience_counter = save_best_model(val_auc_fold1, best_val_auc_fold1, "fold1")
            best_val_auc_fold2, patience_counter = save_best_model(val_auc_fold2, best_val_auc_fold2, "fold2")
            best_val_auc_all, patience_counter = save_best_model(val_auc_all, best_val_auc_all, "overall")

            if patience_counter >= args.early_stopping_patience:
                print(f"Early stopping triggered after {epoch + 1} epochs")
                break

    if args.local_rank in [-1, 0]:
        print("\nTraining complete.")
        print(f"Best Validation AUC (Fold 1):  {best_val_auc_fold1:.4f}")
        print(f"Best Validation AUC (Fold 2):  {best_val_auc_fold2:.4f}")
        print(f"Best Validation AUC (Overall): {best_val_auc_all:.4f}")


# ----------------------------------------------------------------------
# Distributed Helper
# ----------------------------------------------------------------------

def gather_predictions(predictions):
    """Gather predictions across distributed processes."""
    if not dist.is_initialized():
        return predictions
    predictions = torch.tensor(predictions).cuda()
    gathered = [torch.zeros_like(predictions) for _ in range(dist.get_world_size())]
    dist.all_gather(gathered, predictions)
    return torch.cat(gathered).cpu().numpy()


# ----------------------------------------------------------------------
# Entry Point
# ----------------------------------------------------------------------

if __name__ == "__main__":
    args = parse_args()
    train_model(args)

